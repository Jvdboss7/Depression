{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_depression_detector.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm1Ba7FR5nKO"
      },
      "source": [
        "### **Project Showcase - Depression Detection using Twitter Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFIRyYt97knb"
      },
      "source": [
        "This script contains pre-processing for the twitter data and model creation using PyTorch. \n",
        "\n",
        "We used TorchText, a PyTorch libray that made pre-pre-processing both simple and efficient, and applied custom techniques to work with our unique twitter data.\n",
        "\n",
        "A lot of the preprocessing and model section was inspired by code from this article, as we are new to NLP using PyTorch - https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlrX5NJiBE8G"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P-BoaT5LJPx",
        "outputId": "ef91b338-8c2c-4c44-c432-4142a3749085"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzadFKTlP0wU",
        "outputId": "c25ca367-3f60-455b-db68-e4861e215629"
      },
      "source": [
        "import seaborn as sns\n",
        "import spacy\n",
        "from tqdm import tqdm, tqdm_notebook, tnrange\n",
        "tqdm.pandas(desc='Progress')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obEvYhLvLva_",
        "outputId": "beeb243f-8683-4c72-8e9b-836af6d826a7"
      },
      "source": [
        "!pip install torchtext==0.4"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████▏                         | 10kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.4) (3.7.4.3)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "Successfully installed torchtext-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCm2NnqrP4fL"
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSy7O88mQMhU"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DkH-8NbQUiU"
      },
      "source": [
        "import os, sys\n",
        "import re\n",
        "import string\n",
        "import itertools"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1fswumgQZGs"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfAbtdwOQa6s"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht17_lvKQpun",
        "outputId": "62523a3c-cfec-4528-f15d-a2db3e0a94ae"
      },
      "source": [
        "print('Python version:',sys.version)\n",
        "print('Pandas version:',pd.__version__)\n",
        "print('Pytorch version:', torch.__version__)\n",
        "print('Torch Text version:', torchtext.__version__)\n",
        "print('Spacy version:', spacy.__version__)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version: 3.7.10 (default, May  3 2021, 02:48:31) \n",
            "[GCC 7.5.0]\n",
            "Pandas version: 1.1.5\n",
            "Pytorch version: 1.8.1+cu101\n",
            "Torch Text version: 0.4.0\n",
            "Spacy version: 2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Cy0aZfvPE0"
      },
      "source": [
        "## **1. Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lvtAgdT7gPN"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Dataset1/depression_data/tweets_combined.csv\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPVE4THcCGId",
        "outputId": "16ce00bd-3e76-4898-fd0a-450f08071d6d"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "yXa79WdbBDPh",
        "outputId": "78d41834-6278-4f74-a1f1-852865531622"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2444</td>\n",
              "      <td>PUPPY FOR ADOPTIONS !!! Happy pics!! Please SHARE for exposure! Put out the word to good adopter prospects!...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1073</td>\n",
              "      <td>Do you ever just get so excited that you're trying to sleep &amp;amp; just can't even close you eyes😁like I really need this catnap but #excitement</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1355</td>\n",
              "      <td>@DOBrienAJC What did you do to rile up all the Roberts?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1738</td>\n",
              "      <td>I didn't give @chelsopat @s0ccer15 credit for bringing me chocolate milk so here it is 🙏🏼 #grateful</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1942</td>\n",
              "      <td>4th step to overcoming  and  is to accept you need help, if you could have done it yourself, you would have, true?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... target\n",
              "0  2444        ...  0    \n",
              "1  1073        ...  0    \n",
              "2  1355        ...  0    \n",
              "3  1738        ...  0    \n",
              "4  1942        ...  0    \n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMpkcMGEBqO5",
        "outputId": "d9d48684-e566-4755-a785-d431ddacb342"
      },
      "source": [
        "df.target.value_counts()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    843\n",
              "0    843\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "3x_LwHt-Pqm9",
        "outputId": "fd117035-51c0-4b55-8b14-7cc352a962fd"
      },
      "source": [
        "fig = plt.figure(figsize=(5,3))\n",
        "ax = sns.barplot(x=df.target.unique(),y=df.target.value_counts());\n",
        "ax.set(xlabel='Labels');"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQfsoDz286Tz",
        "outputId": "8bdfef25-bfde-4632-e762-3601bf7e48be"
      },
      "source": [
        "df.tweet.head(10), df.tweet.tail(10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0    PUPPY FOR ADOPTIONS !!! Happy pics!! Please SHARE for exposure! Put out the word to good adopter prospects!...                                 \n",
              " 1    Do you ever just get so excited that you're trying to sleep &amp; just can't even close you eyes😁like I really need this catnap but #excitement\n",
              " 2    @DOBrienAJC What did you do to rile up all the Roberts?                                                                                        \n",
              " 3    I didn't give @chelsopat @s0ccer15 credit for bringing me chocolate milk so here it is 🙏🏼 #grateful                                            \n",
              " 4    4th step to overcoming  and  is to accept you need help, if you could have done it yourself, you would have, true?                             \n",
              " 5    You do NOT have to be a victim of  or . Change is easier than you may think; you just have to go about it the right way.                       \n",
              " 6    @Dansgaming YES!! I love making my own coffee drinks at home! #super  !!                                                                       \n",
              " 7    @titsjpeg Yeah funny and sexy 😍                                                                                                                \n",
              " 8    had noodles for tea\\nyummmmmmy\\n#yum #yummy #ddlg #abdl #chgl #noodles                                                                         \n",
              " 9     little miss Debbie downer over here.                                                                                                          \n",
              " Name: tweet, dtype: object,\n",
              " 1676    Have to work everyday for the rest of the year                                                                                         \n",
              " 1677    I honestly don't even wanna go to work.                                                                                                \n",
              " 1678    My biggest wish is to one day have a true friend. To be accepted the way I am.                                                         \n",
              " 1679    Might as well just sleep  ???�                                                                                                         \n",
              " 1680    Someone has told me, \"You have very pretty eyes and a smile.. but it's hard to ignore the Hurt, sadness & disappointment behind it.\" ??\n",
              " 1681    Just want to stay in bed all day...                                                                                                    \n",
              " 1682    Doing absolutely fuck all tonight :(                                                                                                   \n",
              " 1683    I need to cuddle someone sooooo bad!                                                                                                   \n",
              " 1684    I am going to try and get some sleep. Not sure if i will be on tomorrow,                                                               \n",
              " 1685    Today I ate 1 month old rancid meat because I forgot what month it was.   .                                                            \n",
              " Name: tweet, dtype: object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xopy9Qr2i6eD",
        "outputId": "453fcda0-6ce9-4172-bd8f-e11a36ae3450"
      },
      "source": [
        "# check non-depressive tweets\n",
        "df[df[\"target\"]==0].tweet.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    PUPPY FOR ADOPTIONS !!! Happy pics!! Please SHARE for exposure! Put out the word to good adopter prospects!...                                 \n",
              "1    Do you ever just get so excited that you're trying to sleep &amp; just can't even close you eyes😁like I really need this catnap but #excitement\n",
              "2    @DOBrienAJC What did you do to rile up all the Roberts?                                                                                        \n",
              "3    I didn't give @chelsopat @s0ccer15 credit for bringing me chocolate milk so here it is 🙏🏼 #grateful                                            \n",
              "4    4th step to overcoming  and  is to accept you need help, if you could have done it yourself, you would have, true?                             \n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-OqFJlvBs2B",
        "outputId": "17cb939f-ce3e-42ca-fb47-88aedec702e7"
      },
      "source": [
        "# check depressive tweets\n",
        "df[df[\"target\"]==1].tweet.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "843    Lately - nothing has been going right .                                      \n",
              "844    My brain caught a bug, Or maybe it was my heart, Darkness gives a fucj       \n",
              "845    I am so  that I am  today                                                    \n",
              "846    No I'm not  for my life.                                                     \n",
              "847    Me: My \"check engine\" light is on. Mechanic: Which car? Me: No I said mine.  \n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydm2HSMOwBSQ"
      },
      "source": [
        "## **2. Define How to Preprocess Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts4ioqHeRCC8",
        "outputId": "ee9eaca2-3d99-42b9-9ede-5e2cac2649a5"
      },
      "source": [
        "# torchtext have trouble handling \\n. Replace \\n character with space\n",
        "df['tweet'] = df.tweet.progress_apply(lambda x: re.sub('\\n', ' ', x))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: 100%|██████████| 1686/1686 [00:00<00:00, 157939.80it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo57ODfhRWO0"
      },
      "source": [
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy7D7zhvpsvh"
      },
      "source": [
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc_P0uFHkYJh"
      },
      "source": [
        "def tweet_clean(text):\n",
        "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove urls\n",
        "    text = re.sub(r'<([^>]*)>', ' ', text) # remove emojis\n",
        "    text = re.sub(r'@\\w+', ' ', text) # remove at mentions\n",
        "    text = re.sub(r'#', '', text) # remove hashtag symbol\n",
        "    text = re.sub(r'[0-9]+', ' ', text) # remove numbers\n",
        "    text = replace_contractions(text)\n",
        "    pattern = re.compile(r\"[ \\n\\t]+\")\n",
        "    text = pattern.sub(\" \", text)      \n",
        "    text = \"\".join(\"\".join(s)[:2] for _, s in itertools.groupby(text))    \n",
        "    text = re.sub(r'[^A-Za-z0-9,?.!]+', ' ', text) # remove all symbols and punctuation except for . , ! and ?\n",
        "    return text.strip()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCE-ztcy0RFq"
      },
      "source": [
        "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
        "def tokenizer(s): return [w.text.lower() for w in nlp(tweet_clean(s))]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5c4RdL90Wgu"
      },
      "source": [
        "**Define fields**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFlQw-Pt05eH"
      },
      "source": [
        "TEXT = Field(sequential=True, tokenize=tokenizer, include_lengths=True, use_vocab=True)\n",
        "TARGET = Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None, is_target =False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXS_2wDd0qQ9"
      },
      "source": [
        "data_fields = [\n",
        "    (None, None),\n",
        "    (\"tweet\", TEXT), \n",
        "    (\"target\", TARGET)\n",
        "]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJHSyAjrxsEc"
      },
      "source": [
        "## **3. Create Train, Valid and Test datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnN_75qpgMHN"
      },
      "source": [
        "def split_train_test(df, test_size=0.2):\n",
        "    train, val = train_test_split(df, test_size=test_size,random_state=42)\n",
        "    return train.reset_index(drop=True), val.reset_index(drop=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEFA4z2CE_Sq"
      },
      "source": [
        "# create train and validation set \n",
        "train_val, test = split_train_test(df, test_size=0.2)\n",
        "train, val = split_train_test(train_val, test_size=0.2)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19vn3wuLFeae"
      },
      "source": [
        "train.to_csv(\"train.csv\", index=False)\n",
        "val.to_csv(\"val.csv\", index=False)\n",
        "test.to_csv(\"test.csv\", index=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFi9p4ibRvX0",
        "outputId": "74bb864f-9574-4cfb-9042-301f62264530"
      },
      "source": [
        "train.shape, val.shape, test.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1078, 3), (270, 3), (338, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "HC87TsYER-0Q",
        "outputId": "533b196b-5edd-4fdd-f23c-e729434627b5"
      },
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "ax = fig.add_subplot(1,3,1)\n",
        "ax = sns.barplot(x=train.target.unique(),y=train.target.value_counts())\n",
        "ax.set(xlabel='Labels', ylabel=\"counts\", title=\"train\")\n",
        "\n",
        "ax1 = fig.add_subplot(1,3,2)\n",
        "ax1 = sns.barplot(x=val.target.unique(),y=val.target.value_counts())\n",
        "ax1.set(xlabel='Labels', ylabel=\"counts\", title=\"validation\")\n",
        "\n",
        "ax2 = fig.add_subplot(1,3,3)\n",
        "ax2 = sns.barplot(x=test.target.unique(),y=test.target.value_counts())\n",
        "ax2.set(xlabel='Labels', ylabel=\"counts\", title=\"test\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0.5, 'counts'), Text(0.5, 0, 'Labels'), Text(0.5, 1.0, 'test')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uswTGPRH2v6J",
        "outputId": "3746558c-c66b-4be5-d4fa-1190557de18e"
      },
      "source": [
        "%%time\n",
        "train_data, val_data, test_data = TabularDataset.splits(path='./', format='csv', train='train.csv', validation='val.csv', test='test.csv', fields=data_fields, skip_header=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 47s, sys: 1.72 s, total: 1min 49s\n",
            "Wall time: 1min 49s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSgxCIC23keA",
        "outputId": "b8502959-2642-4056-b54d-02d72f30b8b2"
      },
      "source": [
        "len(train_data), len(val_data), len(test_data)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1078, 270, 338)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z54cb2_B4b79"
      },
      "source": [
        "## **4. Load pretrained embeddings and build vocab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-tsm29k_WTq",
        "outputId": "77ce3c4a-17bb-43b9-9117-8a1afe22c954"
      },
      "source": [
        "!ls '/content/gdrive/My Drive/embedding'"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/gdrive/My Drive/embedding': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GfaJCDJEBK7",
        "outputId": "05d7d407-1fc7-49d4-a7f4-780b713768c3"
      },
      "source": [
        "%%time\n",
        "vec = torchtext.vocab.Vectors( '/content/drive/MyDrive/Dataset1/depression_data/glove.twitter.27B.100d (1).txt')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1192814/1193514 [00:58<00:00, 20128.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 1s, sys: 4.59 s, total: 1min 5s\n",
            "Wall time: 1min 21s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8HuaHBEod81",
        "outputId": "76e31d2e-9e8b-4fc7-e9bb-52a7c2a2b2cc"
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1078"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGAbh_g2QxAt",
        "outputId": "e6021615-6f80-4229-d7dd-bd8f5aa0625d"
      },
      "source": [
        "%%time\n",
        "MAX_VOCAB_SIZE = 100_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE,\n",
        "                 vectors=vec)\n",
        "\n",
        "TARGET.build_vocab(train_data)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 96.2 ms, sys: 5.98 ms, total: 102 ms\n",
            "Wall time: 106 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaAJsJviSrr2",
        "outputId": "68f0b3da-293b-4915-b703-51384f9b35f2"
      },
      "source": [
        "TEXT.vocab.vectors.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3408, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3vwetTcG9mx",
        "outputId": "7e0152df-449c-46ac-a679-03da08d198b6"
      },
      "source": [
        "train_data"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.data.dataset.TabularDataset at 0x7f9fc9290110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrF-Wlbi5czS"
      },
      "source": [
        "## **5. Load data in batches**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwBCojaA-bk7"
      },
      "source": [
        "We will use the BucketIterator to access the Dataloader. It sorts data according to length of text, and groups similar length text in a batch, thus reducing the amount of padding required. It pads the batch according to the max length in that particular batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25yseUiWKwfC"
      },
      "source": [
        "\n",
        "train_loader, val_loader, test_loader = BucketIterator.splits(datasets=(train_data, val_data, test_data), \n",
        "                                            batch_sizes=(3,3,3), \n",
        "                                            sort_key=lambda x: len(x.tweet), \n",
        "                                            device=None, \n",
        "                                            sort_within_batch=True, \n",
        "                                            repeat=False)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-xV8SAC7dZ7",
        "outputId": "3899aea4-9a61-4dfc-e94d-41db786f9697"
      },
      "source": [
        "len(train_loader), len(val_loader), len(test_loader)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(360, 90, 113)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_p56mLY71w7",
        "outputId": "049f9b67-cacf-4f84-bc95-b3ddbc43e3a9"
      },
      "source": [
        "batch = next(iter(train_loader))\n",
        "type(batch)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchtext.data.batch.Batch"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMp72yoo7-70",
        "outputId": "ad277617-7290-45d0-dbd4-557d7d985362"
      },
      "source": [
        "batch.target"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W5vLVNy81Lj",
        "outputId": "7004f117-2e44-4872-9989-e91041285e3e"
      },
      "source": [
        "batch.tweet"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  10,    6,  163],\n",
              "         [  38,  191,   24],\n",
              "         [ 121,   53,   11],\n",
              "         [ 681,  221,  336],\n",
              "         [   9,    6,  330],\n",
              "         [ 763,  119,   58],\n",
              "         [  21,   53,   38],\n",
              "         [  73,  782,   68],\n",
              "         [  34,    6,  116],\n",
              "         [  46,  147,   11],\n",
              "         [  10,  267,    9],\n",
              "         [ 617,  143,  751],\n",
              "         [ 145,   42, 2910],\n",
              "         [ 290,  267, 2386],\n",
              "         [  14,  248, 1376]]), tensor([15, 15, 15]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d3KJ6U2R9Lbz",
        "outputId": "dd9aa849-23fe-4a32-da7c-173778748f03"
      },
      "source": [
        "TEXT.vocab.itos[1]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<pad>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfABZ4WR9nZT"
      },
      "source": [
        "def idxtosent(batch, idx):\n",
        "    return ' '.join([TEXT.vocab.itos[i] for i in batch.tweet[0][:,idx].cpu().data.numpy()])"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fj5ZIbUC9ydr",
        "outputId": "0e6757ad-1e01-4d35-fed5-623616925a6d"
      },
      "source": [
        "idxtosent(batch,0)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'you all ever heard a song so good but now you ca nt remember it'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzTDu3aR90zC",
        "outputId": "c23c3fd5-9328-4b9b-d919-6902c2f6ce60"
      },
      "source": [
        "batch.__dict__"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 3,\n",
              " 'dataset': <torchtext.data.dataset.TabularDataset at 0x7f9fc9290110>,\n",
              " 'fields': dict_keys([None, 'tweet', 'target']),\n",
              " 'input_fields': ['tweet', 'target'],\n",
              " 'target': tensor([0, 0, 0]),\n",
              " 'target_fields': [],\n",
              " 'tweet': (tensor([[  10,    6,  163],\n",
              "          [  38,  191,   24],\n",
              "          [ 121,   53,   11],\n",
              "          [ 681,  221,  336],\n",
              "          [   9,    6,  330],\n",
              "          [ 763,  119,   58],\n",
              "          [  21,   53,   38],\n",
              "          [  73,  782,   68],\n",
              "          [  34,    6,  116],\n",
              "          [  46,  147,   11],\n",
              "          [  10,  267,    9],\n",
              "          [ 617,  143,  751],\n",
              "          [ 145,   42, 2910],\n",
              "          [ 290,  267, 2386],\n",
              "          [  14,  248, 1376]]), tensor([15, 15, 15]))}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClH37CUB-GXT"
      },
      "source": [
        "class BatchGenerator:\n",
        "    def __init__(self, dl, x_field, y_field):\n",
        "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for batch in self.dl:\n",
        "            X = getattr(batch, self.x_field)\n",
        "            y = getattr(batch, self.y_field)\n",
        "            yield (X,y)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYWXjqv--UEF",
        "outputId": "2a1bc4bd-d1bf-42f4-8ce7-152e97d1ca9b"
      },
      "source": [
        "train_batch_it = BatchGenerator(train_loader, 'tweet', 'target')\n",
        "next(iter(train_batch_it))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([[  14,   74,   45],\n",
              "          [ 647,   11, 1103],\n",
              "          [  62,    9,    5],\n",
              "          [ 136,  215,  232],\n",
              "          [  24,  168,   99],\n",
              "          [   2,    2,   64],\n",
              "          [ 248,   19,  312],\n",
              "          [  32,   13, 1700],\n",
              "          [  12,   56,    7],\n",
              "          [ 169,  326,  162],\n",
              "          [  17,    3,   31],\n",
              "          [   9,  127,   64],\n",
              "          [1387,   49,   14],\n",
              "          [ 479,    2,   38],\n",
              "          [   6,   20,   10],\n",
              "          [3012,    9,   57],\n",
              "          [  15,  298, 1406],\n",
              "          [   9,   15,   16],\n",
              "          [3097, 1992,   16],\n",
              "          [2359,    7,   34],\n",
              "          [   3,  875,    2],\n",
              "          [   2,    7,   27],\n",
              "          [  66,  343, 1113],\n",
              "          [   5,    7,   57],\n",
              "          [ 222, 2018,    5],\n",
              "          [   9,    7,   55],\n",
              "          [1441,    2,    5],\n",
              "          [   7,   31,   68],\n",
              "          [ 112,   13,   12],\n",
              "          [ 479,  136,  169],\n",
              "          [   3,   14,  902],\n",
              "          [   7,    3,   37]]), tensor([32, 32, 32])), tensor([1, 0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj_aJPx__mv5"
      },
      "source": [
        "## **6. Models and Training**\n",
        "\n",
        "For the model, we decided to follow the example in https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8, but make small modifications such as adding some dropout, to prevent overfitting\n",
        "\n",
        "The model is uses a pre-trained embedding layer from glove, a bidirectional GRU and also a concat pooling method where we perform average pool and max pool and then concatenate the results.\n",
        "\n",
        "The final result was ok, with around 80% test accuracy. It was clear that the model was overfitting but we had run out of time to make further adjustments. This has been a very educational experience as it was our first time implementing NLP using PyTorch. We plan to experiment and improve the model using a varitey of methods in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMfwXhUUGBcD"
      },
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "n_hidden = 64\n",
        "n_out = 2"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-puOJHXXkcL0"
      },
      "source": [
        "class ConcatPoolingGRUAdaptive(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_hidden, n_out, pretrained_vec, dropout, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_out = n_out\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        self.emb = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.emb.weight.data.copy_(pretrained_vec)\n",
        "        self.emb.weight.requires_grad = False\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.n_hidden, bidirectional=bidirectional)\n",
        "        if bidirectional:\n",
        "            self.fc = nn.Linear(self.n_hidden*2*2, self.n_out)\n",
        "        else:\n",
        "            self.fc = nn.Linear(self.n_hidden*2, self.n_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, seq, lengths):\n",
        "        bs = seq.size(1)\n",
        "        self.h = self.init_hidden(bs)\n",
        "        seq = seq.transpose(0,1)\n",
        "        embs = self.emb(seq)\n",
        "        embs = embs.transpose(0,1)\n",
        "        embs = pack_padded_sequence(embs, lengths)\n",
        "        gru_out, self.h = self.gru(embs, self.h)\n",
        "        gru_out, lengths = pad_packed_sequence(gru_out)        \n",
        "        \n",
        "        avg_pool = F.adaptive_avg_pool1d(gru_out.permute(1,2,0),1).view(bs,-1)\n",
        "        max_pool = F.adaptive_max_pool1d(gru_out.permute(1,2,0),1).view(bs,-1) \n",
        "        \n",
        "        cat = self.dropout(torch.cat([avg_pool,max_pool],dim=1))\n",
        "        \n",
        "        outp = self.fc(cat)\n",
        "        return F.log_softmax(outp)\n",
        "    \n",
        "    def init_hidden(self, batch_size): \n",
        "        if self.bidirectional:\n",
        "            return torch.zeros((2,batch_size,self.n_hidden)).to(device)\n",
        "        else:\n",
        "            return torch.zeros((1,batch_size,self.n_hidden)).cuda().to(device)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlycNBIGGtO5"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, num_batch):\n",
        "        y_true_train = list()\n",
        "        y_pred_train = list()\n",
        "        total_loss_train = 0  \n",
        "  \n",
        "        #t = tqdm_notebook(iterator, leave=False, total=num_batch)\n",
        "    \n",
        "        for (X,lengths),y in iterator:\n",
        "\n",
        "          #t.set_description(f'Epoch {epoch}')\n",
        "          lengths = lengths.cpu().numpy()\n",
        "\n",
        "          opt.zero_grad()\n",
        "          pred = model(X, lengths)\n",
        "          loss = criterion(pred, y)\n",
        "          loss.backward()\n",
        "          opt.step()\n",
        "\n",
        "          #t.set_postfix(loss=loss.item())\n",
        "          pred_idx = torch.max(pred, dim=1)[1]\n",
        "\n",
        "          y_true_train += list(y.cpu().data.numpy())\n",
        "          y_pred_train += list(pred_idx.cpu().data.numpy())\n",
        "          total_loss_train += loss.item()\n",
        "            \n",
        "        train_acc = accuracy_score(y_true_train, y_pred_train)\n",
        "        train_loss = total_loss_train/num_batch\n",
        "        return train_loss, train_acc"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j74LaFc6HlRU"
      },
      "source": [
        "def evaluate(model, iterator, criterion, num_batch):\n",
        "            y_true_val = list()\n",
        "            y_pred_val = list()\n",
        "            total_loss_val = 0\n",
        "            for (X,lengths),y in iterator: #tqdm_notebook(iterator, leave=False): \n",
        "            \n",
        "              pred = model(X, lengths.cpu().numpy())\n",
        "              loss = criterion(pred, y)\n",
        "              pred_idx = torch.max(pred, 1)[1]\n",
        "              y_true_val += list(y.cpu().data.numpy())\n",
        "              y_pred_val += list(pred_idx.cpu().data.numpy())\n",
        "              total_loss_val += loss.item()\n",
        "            valacc = accuracy_score(y_true_val, y_pred_val)\n",
        "            valloss = total_loss_val/num_batch\n",
        "            return valloss, valacc\n",
        "         "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq402kPY_086"
      },
      "source": [
        "train_loader, val_loader, test_loader = BucketIterator.splits(datasets=(train_data, val_data, test_data), \n",
        "                                            batch_sizes=(32,32,32), \n",
        "                                            sort_key=lambda x: len(x.tweet), \n",
        "                                            device=device, \n",
        "                                            sort_within_batch=True, \n",
        "                                            repeat=False)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOoo-vYpFLOi"
      },
      "source": [
        "train_batch_it = BatchGenerator(train_loader, 'tweet', 'target')\n",
        "val_batch_it = BatchGenerator(val_loader, 'tweet', 'target')\n",
        "test_batch_it = BatchGenerator(test_loader, 'tweet', 'target')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c8x62WYvD4o"
      },
      "source": [
        "m = ConcatPoolingGRUAdaptive(vocab_size, embedding_dim, n_hidden, n_out, train_data.fields['tweet'].vocab.vectors, 0.5).to(device)\n",
        "opt = optim.Adam(filter(lambda p: p.requires_grad, m.parameters()), 1e-3)\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APwJlE7QPSO3"
      },
      "source": [
        "loss_fn=F.nll_loss\n",
        "epochs=10"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGtekmstQVWo"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KK44r2qHDZV",
        "outputId": "7648f164-1bec-4775-b328-6e9959c7c9a9"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "epochs=500\n",
        "\n",
        "for epoch in range(epochs):      \n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(m, iter(train_batch_it), opt, loss_fn, len(train_batch_it))\n",
        "    valid_loss, valid_acc = evaluate(m, iter(val_batch_it), loss_fn, len(val_batch_it))\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(m.state_dict(), 'tut4-model.pt')\n",
        "          \n",
        "        \n",
        "    print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {valid_loss:.4f} val_acc: {valid_acc:.4f}')\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: train_loss: 0.6782 train_acc: 0.5668 | val_loss: 0.6445 val_acc: 0.6519\n",
            "Epoch 1: train_loss: 0.6025 train_acc: 0.7013 | val_loss: 0.5887 val_acc: 0.7148\n",
            "Epoch 2: train_loss: 0.5425 train_acc: 0.7347 | val_loss: 0.5155 val_acc: 0.7296\n",
            "Epoch 3: train_loss: 0.4998 train_acc: 0.7662 | val_loss: 0.5580 val_acc: 0.7000\n",
            "Epoch 4: train_loss: 0.4877 train_acc: 0.7690 | val_loss: 0.5131 val_acc: 0.7481\n",
            "Epoch 5: train_loss: 0.4580 train_acc: 0.7848 | val_loss: 0.4830 val_acc: 0.7556\n",
            "Epoch 6: train_loss: 0.4442 train_acc: 0.7987 | val_loss: 0.4971 val_acc: 0.7333\n",
            "Epoch 7: train_loss: 0.4105 train_acc: 0.8098 | val_loss: 0.5007 val_acc: 0.7444\n",
            "Epoch 8: train_loss: 0.3970 train_acc: 0.8312 | val_loss: 0.4917 val_acc: 0.7741\n",
            "Epoch 9: train_loss: 0.3524 train_acc: 0.8469 | val_loss: 0.5020 val_acc: 0.7481\n",
            "Epoch 10: train_loss: 0.3389 train_acc: 0.8562 | val_loss: 0.5245 val_acc: 0.7222\n",
            "Epoch 11: train_loss: 0.3267 train_acc: 0.8618 | val_loss: 0.5381 val_acc: 0.7481\n",
            "Epoch 12: train_loss: 0.3203 train_acc: 0.8609 | val_loss: 0.4998 val_acc: 0.7407\n",
            "Epoch 13: train_loss: 0.2782 train_acc: 0.8915 | val_loss: 0.5015 val_acc: 0.7370\n",
            "Epoch 14: train_loss: 0.2586 train_acc: 0.8998 | val_loss: 0.5225 val_acc: 0.7407\n",
            "Epoch 15: train_loss: 0.2560 train_acc: 0.9026 | val_loss: 0.5177 val_acc: 0.7519\n",
            "Epoch 16: train_loss: 0.2345 train_acc: 0.8915 | val_loss: 0.5525 val_acc: 0.7333\n",
            "Epoch 17: train_loss: 0.1951 train_acc: 0.9360 | val_loss: 0.5952 val_acc: 0.7000\n",
            "Epoch 18: train_loss: 0.1749 train_acc: 0.9351 | val_loss: 0.6233 val_acc: 0.7519\n",
            "Epoch 19: train_loss: 0.1387 train_acc: 0.9453 | val_loss: 0.6652 val_acc: 0.7296\n",
            "Epoch 20: train_loss: 0.1288 train_acc: 0.9508 | val_loss: 0.6943 val_acc: 0.7259\n",
            "Epoch 21: train_loss: 0.1645 train_acc: 0.9378 | val_loss: 0.5878 val_acc: 0.7296\n",
            "Epoch 22: train_loss: 0.1497 train_acc: 0.9425 | val_loss: 0.7302 val_acc: 0.6963\n",
            "Epoch 23: train_loss: 0.1233 train_acc: 0.9536 | val_loss: 0.7450 val_acc: 0.7074\n",
            "Epoch 24: train_loss: 0.0818 train_acc: 0.9777 | val_loss: 0.8183 val_acc: 0.6926\n",
            "Epoch 25: train_loss: 0.0622 train_acc: 0.9824 | val_loss: 0.8458 val_acc: 0.7259\n",
            "Epoch 26: train_loss: 0.0466 train_acc: 0.9898 | val_loss: 0.9488 val_acc: 0.7037\n",
            "Epoch 27: train_loss: 0.0394 train_acc: 0.9907 | val_loss: 0.9566 val_acc: 0.7037\n",
            "Epoch 28: train_loss: 0.0463 train_acc: 0.9870 | val_loss: 0.9121 val_acc: 0.7222\n",
            "Epoch 29: train_loss: 0.0588 train_acc: 0.9861 | val_loss: 0.9435 val_acc: 0.7185\n",
            "Epoch 30: train_loss: 0.0282 train_acc: 0.9963 | val_loss: 1.0292 val_acc: 0.7074\n",
            "Epoch 31: train_loss: 0.0197 train_acc: 0.9991 | val_loss: 1.0867 val_acc: 0.7259\n",
            "Epoch 32: train_loss: 0.0188 train_acc: 0.9972 | val_loss: 1.0733 val_acc: 0.7259\n",
            "Epoch 33: train_loss: 0.0156 train_acc: 0.9991 | val_loss: 1.2218 val_acc: 0.7074\n",
            "Epoch 34: train_loss: 0.0148 train_acc: 0.9991 | val_loss: 1.2888 val_acc: 0.7037\n",
            "Epoch 35: train_loss: 0.0102 train_acc: 1.0000 | val_loss: 1.2256 val_acc: 0.7074\n",
            "Epoch 36: train_loss: 0.0199 train_acc: 0.9954 | val_loss: 1.1487 val_acc: 0.7296\n",
            "Epoch 37: train_loss: 0.0150 train_acc: 0.9963 | val_loss: 1.2630 val_acc: 0.7259\n",
            "Epoch 38: train_loss: 0.1148 train_acc: 0.9499 | val_loss: 0.7766 val_acc: 0.7593\n",
            "Epoch 39: train_loss: 0.0594 train_acc: 0.9805 | val_loss: 0.8289 val_acc: 0.7519\n",
            "Epoch 40: train_loss: 0.0234 train_acc: 0.9963 | val_loss: 0.9105 val_acc: 0.7704\n",
            "Epoch 41: train_loss: 0.0116 train_acc: 0.9991 | val_loss: 1.0530 val_acc: 0.7333\n",
            "Epoch 42: train_loss: 0.0135 train_acc: 0.9972 | val_loss: 1.0959 val_acc: 0.7296\n",
            "Epoch 43: train_loss: 0.0114 train_acc: 0.9991 | val_loss: 1.0151 val_acc: 0.7370\n",
            "Epoch 44: train_loss: 0.0099 train_acc: 0.9981 | val_loss: 1.0939 val_acc: 0.7333\n",
            "Epoch 45: train_loss: 0.0086 train_acc: 0.9981 | val_loss: 1.1207 val_acc: 0.7259\n",
            "Epoch 46: train_loss: 0.0087 train_acc: 0.9981 | val_loss: 1.1994 val_acc: 0.7259\n",
            "Epoch 47: train_loss: 0.0060 train_acc: 0.9991 | val_loss: 1.2607 val_acc: 0.7037\n",
            "Epoch 48: train_loss: 0.0102 train_acc: 0.9981 | val_loss: 1.2285 val_acc: 0.7333\n",
            "Epoch 49: train_loss: 0.0070 train_acc: 0.9981 | val_loss: 1.3303 val_acc: 0.7222\n",
            "Epoch 50: train_loss: 0.0086 train_acc: 0.9991 | val_loss: 1.2289 val_acc: 0.7296\n",
            "Epoch 51: train_loss: 0.0053 train_acc: 0.9991 | val_loss: 1.2796 val_acc: 0.7296\n",
            "Epoch 52: train_loss: 0.0049 train_acc: 0.9991 | val_loss: 1.3377 val_acc: 0.7222\n",
            "Epoch 53: train_loss: 0.0066 train_acc: 0.9981 | val_loss: 1.2511 val_acc: 0.7074\n",
            "Epoch 54: train_loss: 0.0046 train_acc: 0.9991 | val_loss: 1.3997 val_acc: 0.7185\n",
            "Epoch 55: train_loss: 0.0063 train_acc: 0.9981 | val_loss: 1.4541 val_acc: 0.7222\n",
            "Epoch 56: train_loss: 0.0076 train_acc: 0.9991 | val_loss: 1.3727 val_acc: 0.7333\n",
            "Epoch 57: train_loss: 0.0073 train_acc: 0.9991 | val_loss: 1.5229 val_acc: 0.6963\n",
            "Epoch 58: train_loss: 0.0079 train_acc: 0.9981 | val_loss: 1.4319 val_acc: 0.7185\n",
            "Epoch 59: train_loss: 0.0063 train_acc: 0.9991 | val_loss: 1.3623 val_acc: 0.7185\n",
            "Epoch 60: train_loss: 0.0045 train_acc: 0.9981 | val_loss: 1.3646 val_acc: 0.7185\n",
            "Epoch 61: train_loss: 0.0029 train_acc: 1.0000 | val_loss: 1.2967 val_acc: 0.7519\n",
            "Epoch 62: train_loss: 0.0028 train_acc: 0.9991 | val_loss: 1.3650 val_acc: 0.7296\n",
            "Epoch 63: train_loss: 0.0036 train_acc: 0.9981 | val_loss: 1.4077 val_acc: 0.7111\n",
            "Epoch 64: train_loss: 0.0042 train_acc: 0.9991 | val_loss: 1.4679 val_acc: 0.7037\n",
            "Epoch 65: train_loss: 0.0042 train_acc: 0.9991 | val_loss: 1.4410 val_acc: 0.7259\n",
            "Epoch 66: train_loss: 0.0022 train_acc: 1.0000 | val_loss: 1.4592 val_acc: 0.7222\n",
            "Epoch 67: train_loss: 0.0025 train_acc: 0.9991 | val_loss: 1.5152 val_acc: 0.7185\n",
            "Epoch 68: train_loss: 0.0034 train_acc: 0.9981 | val_loss: 1.5686 val_acc: 0.7037\n",
            "Epoch 69: train_loss: 0.0032 train_acc: 0.9991 | val_loss: 1.5581 val_acc: 0.6963\n",
            "Epoch 70: train_loss: 0.0056 train_acc: 0.9991 | val_loss: 1.4941 val_acc: 0.7185\n",
            "Epoch 71: train_loss: 0.0029 train_acc: 0.9991 | val_loss: 1.5278 val_acc: 0.7074\n",
            "Epoch 72: train_loss: 0.0021 train_acc: 0.9991 | val_loss: 1.4926 val_acc: 0.7222\n",
            "Epoch 73: train_loss: 0.0025 train_acc: 0.9991 | val_loss: 1.5827 val_acc: 0.7222\n",
            "Epoch 74: train_loss: 0.0030 train_acc: 0.9991 | val_loss: 1.5061 val_acc: 0.7444\n",
            "Epoch 75: train_loss: 0.0066 train_acc: 0.9991 | val_loss: 1.4693 val_acc: 0.7556\n",
            "Epoch 76: train_loss: 0.0048 train_acc: 0.9981 | val_loss: 1.5047 val_acc: 0.7333\n",
            "Epoch 77: train_loss: 0.0048 train_acc: 0.9981 | val_loss: 1.5829 val_acc: 0.7333\n",
            "Epoch 78: train_loss: 0.0036 train_acc: 0.9991 | val_loss: 1.4239 val_acc: 0.7519\n",
            "Epoch 79: train_loss: 0.0078 train_acc: 0.9972 | val_loss: 1.3276 val_acc: 0.7444\n",
            "Epoch 80: train_loss: 0.0109 train_acc: 0.9972 | val_loss: 1.3891 val_acc: 0.7333\n",
            "Epoch 81: train_loss: 0.0046 train_acc: 0.9991 | val_loss: 1.4937 val_acc: 0.7407\n",
            "Epoch 82: train_loss: 0.0085 train_acc: 0.9981 | val_loss: 1.5168 val_acc: 0.7407\n",
            "Epoch 83: train_loss: 0.0031 train_acc: 0.9981 | val_loss: 1.5370 val_acc: 0.7296\n",
            "Epoch 84: train_loss: 0.0036 train_acc: 0.9981 | val_loss: 1.5792 val_acc: 0.7000\n",
            "Epoch 85: train_loss: 0.0070 train_acc: 0.9981 | val_loss: 1.5771 val_acc: 0.7667\n",
            "Epoch 86: train_loss: 0.0054 train_acc: 0.9991 | val_loss: 1.6899 val_acc: 0.7074\n",
            "Epoch 87: train_loss: 0.4021 train_acc: 0.8673 | val_loss: 0.9504 val_acc: 0.7185\n",
            "Epoch 88: train_loss: 0.2289 train_acc: 0.9045 | val_loss: 0.6736 val_acc: 0.7778\n",
            "Epoch 89: train_loss: 0.0832 train_acc: 0.9750 | val_loss: 0.7889 val_acc: 0.7481\n",
            "Epoch 90: train_loss: 0.0384 train_acc: 0.9870 | val_loss: 0.9478 val_acc: 0.7259\n",
            "Epoch 91: train_loss: 0.0211 train_acc: 0.9963 | val_loss: 0.8746 val_acc: 0.7333\n",
            "Epoch 92: train_loss: 0.0162 train_acc: 0.9963 | val_loss: 0.9697 val_acc: 0.7296\n",
            "Epoch 93: train_loss: 0.0112 train_acc: 0.9991 | val_loss: 1.0552 val_acc: 0.7296\n",
            "Epoch 94: train_loss: 0.0098 train_acc: 0.9972 | val_loss: 1.0839 val_acc: 0.7296\n",
            "Epoch 95: train_loss: 0.0074 train_acc: 0.9981 | val_loss: 1.1176 val_acc: 0.7407\n",
            "Epoch 96: train_loss: 0.0062 train_acc: 0.9991 | val_loss: 1.1883 val_acc: 0.7407\n",
            "Epoch 97: train_loss: 0.0051 train_acc: 0.9981 | val_loss: 1.2468 val_acc: 0.7148\n",
            "Epoch 98: train_loss: 0.0064 train_acc: 0.9981 | val_loss: 1.3635 val_acc: 0.7333\n",
            "Epoch 99: train_loss: 0.0042 train_acc: 0.9981 | val_loss: 1.3474 val_acc: 0.7407\n",
            "Epoch 100: train_loss: 0.0062 train_acc: 0.9981 | val_loss: 1.3636 val_acc: 0.7222\n",
            "Epoch 101: train_loss: 0.0032 train_acc: 1.0000 | val_loss: 1.2865 val_acc: 0.7444\n",
            "Epoch 102: train_loss: 0.0050 train_acc: 0.9991 | val_loss: 1.4018 val_acc: 0.7148\n",
            "Epoch 103: train_loss: 0.0032 train_acc: 0.9991 | val_loss: 1.3715 val_acc: 0.7222\n",
            "Epoch 104: train_loss: 0.0045 train_acc: 0.9991 | val_loss: 1.4592 val_acc: 0.7259\n",
            "Epoch 105: train_loss: 0.0048 train_acc: 0.9981 | val_loss: 1.4770 val_acc: 0.7259\n",
            "Epoch 106: train_loss: 0.0023 train_acc: 0.9991 | val_loss: 1.4830 val_acc: 0.7111\n",
            "Epoch 107: train_loss: 0.0034 train_acc: 0.9991 | val_loss: 1.5585 val_acc: 0.7407\n",
            "Epoch 108: train_loss: 0.0093 train_acc: 0.9972 | val_loss: 1.2883 val_acc: 0.7370\n",
            "Epoch 109: train_loss: 0.0164 train_acc: 0.9972 | val_loss: 1.2104 val_acc: 0.7444\n",
            "Epoch 110: train_loss: 0.0110 train_acc: 0.9963 | val_loss: 1.4105 val_acc: 0.7444\n",
            "Epoch 111: train_loss: 0.0062 train_acc: 0.9981 | val_loss: 1.3112 val_acc: 0.7407\n",
            "Epoch 112: train_loss: 0.0056 train_acc: 0.9981 | val_loss: 1.3475 val_acc: 0.7333\n",
            "Epoch 113: train_loss: 0.0028 train_acc: 1.0000 | val_loss: 1.4294 val_acc: 0.7296\n",
            "Epoch 114: train_loss: 0.0046 train_acc: 0.9981 | val_loss: 1.4453 val_acc: 0.7370\n",
            "Epoch 115: train_loss: 0.0033 train_acc: 0.9991 | val_loss: 1.4214 val_acc: 0.7000\n",
            "Epoch 116: train_loss: 0.0053 train_acc: 0.9981 | val_loss: 1.4967 val_acc: 0.7370\n",
            "Epoch 117: train_loss: 0.0033 train_acc: 0.9991 | val_loss: 1.4208 val_acc: 0.7556\n",
            "Epoch 118: train_loss: 0.0033 train_acc: 0.9991 | val_loss: 1.3420 val_acc: 0.7444\n",
            "Epoch 119: train_loss: 0.0029 train_acc: 0.9991 | val_loss: 1.5397 val_acc: 0.7222\n",
            "Epoch 120: train_loss: 0.0044 train_acc: 0.9991 | val_loss: 1.4379 val_acc: 0.7370\n",
            "Epoch 121: train_loss: 0.0022 train_acc: 0.9991 | val_loss: 1.4103 val_acc: 0.7593\n",
            "Epoch 122: train_loss: 0.0028 train_acc: 0.9991 | val_loss: 1.5194 val_acc: 0.7333\n",
            "Epoch 123: train_loss: 0.0027 train_acc: 0.9991 | val_loss: 1.5494 val_acc: 0.7407\n",
            "Epoch 124: train_loss: 0.0069 train_acc: 0.9981 | val_loss: 1.5079 val_acc: 0.7259\n",
            "Epoch 125: train_loss: 0.0087 train_acc: 0.9981 | val_loss: 1.8715 val_acc: 0.7667\n",
            "Epoch 126: train_loss: 0.0663 train_acc: 0.9814 | val_loss: 1.2028 val_acc: 0.7222\n",
            "Epoch 127: train_loss: 0.0316 train_acc: 0.9907 | val_loss: 1.3758 val_acc: 0.7259\n",
            "Epoch 128: train_loss: 0.0061 train_acc: 0.9991 | val_loss: 1.4421 val_acc: 0.7407\n",
            "Epoch 129: train_loss: 0.0040 train_acc: 0.9991 | val_loss: 1.4809 val_acc: 0.7148\n",
            "Epoch 130: train_loss: 0.0037 train_acc: 0.9991 | val_loss: 1.4545 val_acc: 0.7222\n",
            "Epoch 131: train_loss: 0.0079 train_acc: 0.9981 | val_loss: 1.4246 val_acc: 0.7222\n",
            "Epoch 132: train_loss: 0.0045 train_acc: 0.9991 | val_loss: 1.5081 val_acc: 0.7296\n",
            "Epoch 133: train_loss: 0.0023 train_acc: 0.9991 | val_loss: 1.5790 val_acc: 0.7333\n",
            "Epoch 134: train_loss: 0.0012 train_acc: 1.0000 | val_loss: 1.6452 val_acc: 0.7148\n",
            "Epoch 135: train_loss: 0.0043 train_acc: 0.9991 | val_loss: 1.4978 val_acc: 0.7370\n",
            "Epoch 136: train_loss: 0.0019 train_acc: 1.0000 | val_loss: 1.5445 val_acc: 0.7185\n",
            "Epoch 137: train_loss: 0.0026 train_acc: 0.9991 | val_loss: 1.5519 val_acc: 0.7148\n",
            "Epoch 138: train_loss: 0.0017 train_acc: 1.0000 | val_loss: 1.6121 val_acc: 0.7259\n",
            "Epoch 139: train_loss: 0.0026 train_acc: 0.9991 | val_loss: 1.5975 val_acc: 0.7370\n",
            "Epoch 140: train_loss: 0.0033 train_acc: 0.9991 | val_loss: 1.6400 val_acc: 0.7407\n",
            "Epoch 141: train_loss: 0.0049 train_acc: 0.9991 | val_loss: 1.7056 val_acc: 0.7111\n",
            "Epoch 142: train_loss: 0.0018 train_acc: 0.9991 | val_loss: 1.6157 val_acc: 0.7185\n",
            "Epoch 143: train_loss: 0.0022 train_acc: 0.9991 | val_loss: 1.5671 val_acc: 0.7370\n",
            "Epoch 144: train_loss: 0.0023 train_acc: 0.9981 | val_loss: 1.6840 val_acc: 0.7148\n",
            "Epoch 145: train_loss: 0.0049 train_acc: 0.9991 | val_loss: 1.7018 val_acc: 0.7111\n",
            "Epoch 146: train_loss: 0.0039 train_acc: 0.9991 | val_loss: 1.7079 val_acc: 0.7370\n",
            "Epoch 147: train_loss: 0.0031 train_acc: 0.9981 | val_loss: 1.6299 val_acc: 0.7370\n",
            "Epoch 148: train_loss: 0.0034 train_acc: 0.9991 | val_loss: 1.7131 val_acc: 0.7185\n",
            "Epoch 149: train_loss: 0.0024 train_acc: 0.9991 | val_loss: 1.6821 val_acc: 0.7444\n",
            "Epoch 150: train_loss: 0.0020 train_acc: 0.9991 | val_loss: 1.6018 val_acc: 0.7185\n",
            "Epoch 151: train_loss: 0.0027 train_acc: 0.9991 | val_loss: 1.6319 val_acc: 0.7333\n",
            "Epoch 152: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 1.6800 val_acc: 0.7259\n",
            "Epoch 153: train_loss: 0.0011 train_acc: 0.9991 | val_loss: 1.7820 val_acc: 0.7222\n",
            "Epoch 154: train_loss: 0.0065 train_acc: 0.9981 | val_loss: 1.7210 val_acc: 0.7185\n",
            "Epoch 155: train_loss: 0.0043 train_acc: 0.9981 | val_loss: 1.7281 val_acc: 0.7370\n",
            "Epoch 156: train_loss: 0.0039 train_acc: 0.9991 | val_loss: 1.7714 val_acc: 0.7148\n",
            "Epoch 157: train_loss: 0.0032 train_acc: 0.9991 | val_loss: 1.7482 val_acc: 0.7296\n",
            "Epoch 158: train_loss: 0.0058 train_acc: 0.9981 | val_loss: 1.7417 val_acc: 0.7333\n",
            "Epoch 159: train_loss: 0.0062 train_acc: 0.9981 | val_loss: 1.7229 val_acc: 0.7111\n",
            "Epoch 160: train_loss: 0.0033 train_acc: 0.9981 | val_loss: 1.7470 val_acc: 0.7407\n",
            "Epoch 161: train_loss: 0.0046 train_acc: 0.9981 | val_loss: 1.7696 val_acc: 0.7074\n",
            "Epoch 162: train_loss: 0.0033 train_acc: 0.9991 | val_loss: 1.7284 val_acc: 0.7333\n",
            "Epoch 163: train_loss: 0.0034 train_acc: 0.9991 | val_loss: 1.7753 val_acc: 0.7222\n",
            "Epoch 164: train_loss: 0.0030 train_acc: 0.9991 | val_loss: 1.7720 val_acc: 0.7148\n",
            "Epoch 165: train_loss: 0.0018 train_acc: 0.9991 | val_loss: 1.7440 val_acc: 0.7370\n",
            "Epoch 166: train_loss: 0.0028 train_acc: 0.9991 | val_loss: 1.7909 val_acc: 0.7185\n",
            "Epoch 167: train_loss: 0.0021 train_acc: 0.9991 | val_loss: 1.6775 val_acc: 0.7222\n",
            "Epoch 168: train_loss: 0.0020 train_acc: 0.9991 | val_loss: 1.6877 val_acc: 0.7296\n",
            "Epoch 169: train_loss: 0.0037 train_acc: 0.9991 | val_loss: 1.8503 val_acc: 0.7185\n",
            "Epoch 170: train_loss: 0.0060 train_acc: 0.9981 | val_loss: 1.7945 val_acc: 0.7185\n",
            "Epoch 171: train_loss: 0.0008 train_acc: 1.0000 | val_loss: 1.6607 val_acc: 0.7111\n",
            "Epoch 172: train_loss: 0.0072 train_acc: 0.9981 | val_loss: 1.7276 val_acc: 0.7407\n",
            "Epoch 173: train_loss: 0.0011 train_acc: 1.0000 | val_loss: 1.7628 val_acc: 0.7222\n",
            "Epoch 174: train_loss: 0.0015 train_acc: 0.9991 | val_loss: 1.7631 val_acc: 0.7111\n",
            "Epoch 175: train_loss: 0.0010 train_acc: 1.0000 | val_loss: 1.7513 val_acc: 0.7333\n",
            "Epoch 176: train_loss: 0.0026 train_acc: 0.9991 | val_loss: 1.8410 val_acc: 0.7000\n",
            "Epoch 177: train_loss: 0.0024 train_acc: 1.0000 | val_loss: 1.5674 val_acc: 0.7185\n",
            "Epoch 178: train_loss: 0.0813 train_acc: 0.9824 | val_loss: 1.6438 val_acc: 0.6852\n",
            "Epoch 179: train_loss: 0.0440 train_acc: 0.9870 | val_loss: 1.0990 val_acc: 0.7333\n",
            "Epoch 180: train_loss: 0.0105 train_acc: 0.9972 | val_loss: 1.3699 val_acc: 0.7296\n",
            "Epoch 181: train_loss: 0.0027 train_acc: 1.0000 | val_loss: 1.3024 val_acc: 0.7296\n",
            "Epoch 182: train_loss: 0.0037 train_acc: 0.9991 | val_loss: 1.4282 val_acc: 0.7148\n",
            "Epoch 183: train_loss: 0.0034 train_acc: 0.9991 | val_loss: 1.4542 val_acc: 0.7370\n",
            "Epoch 184: train_loss: 0.0046 train_acc: 0.9991 | val_loss: 1.3815 val_acc: 0.7296\n",
            "Epoch 185: train_loss: 0.0022 train_acc: 0.9991 | val_loss: 1.5521 val_acc: 0.7222\n",
            "Epoch 186: train_loss: 0.0028 train_acc: 0.9981 | val_loss: 1.5026 val_acc: 0.7222\n",
            "Epoch 187: train_loss: 0.0021 train_acc: 0.9991 | val_loss: 1.5549 val_acc: 0.7148\n",
            "Epoch 188: train_loss: 0.0040 train_acc: 0.9981 | val_loss: 1.5583 val_acc: 0.7185\n",
            "Epoch 189: train_loss: 0.0029 train_acc: 0.9991 | val_loss: 1.5998 val_acc: 0.7148\n",
            "Epoch 190: train_loss: 0.0027 train_acc: 0.9991 | val_loss: 1.6064 val_acc: 0.7222\n",
            "Epoch 191: train_loss: 0.0040 train_acc: 0.9981 | val_loss: 1.6739 val_acc: 0.7074\n",
            "Epoch 192: train_loss: 0.0038 train_acc: 0.9981 | val_loss: 1.6391 val_acc: 0.7037\n",
            "Epoch 193: train_loss: 0.0023 train_acc: 0.9991 | val_loss: 1.6246 val_acc: 0.7259\n",
            "Epoch 194: train_loss: 0.0007 train_acc: 1.0000 | val_loss: 1.6283 val_acc: 0.7148\n",
            "Epoch 195: train_loss: 0.0026 train_acc: 0.9981 | val_loss: 1.7726 val_acc: 0.7185\n",
            "Epoch 196: train_loss: 0.0053 train_acc: 0.9981 | val_loss: 1.6343 val_acc: 0.7259\n",
            "Epoch 197: train_loss: 0.0015 train_acc: 1.0000 | val_loss: 1.6535 val_acc: 0.7370\n",
            "Epoch 198: train_loss: 0.0033 train_acc: 0.9981 | val_loss: 1.6309 val_acc: 0.7148\n",
            "Epoch 199: train_loss: 0.0042 train_acc: 0.9991 | val_loss: 1.7038 val_acc: 0.7111\n",
            "Epoch 200: train_loss: 0.0022 train_acc: 0.9991 | val_loss: 1.7285 val_acc: 0.7148\n",
            "Epoch 201: train_loss: 0.0045 train_acc: 0.9991 | val_loss: 1.6463 val_acc: 0.7222\n",
            "Epoch 202: train_loss: 0.0020 train_acc: 0.9991 | val_loss: 1.7586 val_acc: 0.7111\n",
            "Epoch 203: train_loss: 0.0044 train_acc: 0.9981 | val_loss: 1.6517 val_acc: 0.7185\n",
            "Epoch 204: train_loss: 0.0034 train_acc: 0.9981 | val_loss: 1.7379 val_acc: 0.7000\n",
            "Epoch 205: train_loss: 0.0013 train_acc: 1.0000 | val_loss: 1.7145 val_acc: 0.7259\n",
            "Epoch 206: train_loss: 0.0019 train_acc: 0.9991 | val_loss: 1.7244 val_acc: 0.7407\n",
            "Epoch 207: train_loss: 0.0026 train_acc: 0.9991 | val_loss: 1.7767 val_acc: 0.7222\n",
            "Epoch 208: train_loss: 0.0042 train_acc: 0.9991 | val_loss: 1.7868 val_acc: 0.7185\n",
            "Epoch 209: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.6999 val_acc: 0.7259\n",
            "Epoch 210: train_loss: 0.0016 train_acc: 0.9991 | val_loss: 1.7145 val_acc: 0.7074\n",
            "Epoch 211: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.8375 val_acc: 0.7148\n",
            "Epoch 212: train_loss: 0.0044 train_acc: 0.9981 | val_loss: 1.6529 val_acc: 0.7111\n",
            "Epoch 213: train_loss: 0.0024 train_acc: 0.9991 | val_loss: 1.7494 val_acc: 0.7074\n",
            "Epoch 214: train_loss: 0.0023 train_acc: 0.9981 | val_loss: 1.7064 val_acc: 0.7111\n",
            "Epoch 215: train_loss: 0.0011 train_acc: 1.0000 | val_loss: 1.8514 val_acc: 0.7148\n",
            "Epoch 216: train_loss: 0.0010 train_acc: 1.0000 | val_loss: 1.7502 val_acc: 0.7111\n",
            "Epoch 217: train_loss: 0.0030 train_acc: 0.9991 | val_loss: 1.8054 val_acc: 0.7111\n",
            "Epoch 218: train_loss: 0.0045 train_acc: 0.9991 | val_loss: 1.7591 val_acc: 0.7111\n",
            "Epoch 219: train_loss: 0.0016 train_acc: 0.9991 | val_loss: 1.6246 val_acc: 0.7148\n",
            "Epoch 220: train_loss: 0.0065 train_acc: 0.9981 | val_loss: 1.7735 val_acc: 0.7148\n",
            "Epoch 221: train_loss: 0.0033 train_acc: 0.9991 | val_loss: 1.7735 val_acc: 0.7185\n",
            "Epoch 222: train_loss: 0.0009 train_acc: 1.0000 | val_loss: 1.6495 val_acc: 0.7222\n",
            "Epoch 223: train_loss: 0.0028 train_acc: 0.9981 | val_loss: 1.6245 val_acc: 0.7185\n",
            "Epoch 224: train_loss: 0.0006 train_acc: 1.0000 | val_loss: 1.7562 val_acc: 0.7148\n",
            "Epoch 225: train_loss: 0.0011 train_acc: 0.9991 | val_loss: 1.6880 val_acc: 0.7333\n",
            "Epoch 226: train_loss: 0.0008 train_acc: 1.0000 | val_loss: 1.7695 val_acc: 0.7037\n",
            "Epoch 227: train_loss: 0.0008 train_acc: 1.0000 | val_loss: 1.7551 val_acc: 0.7370\n",
            "Epoch 228: train_loss: 0.0019 train_acc: 0.9991 | val_loss: 1.7400 val_acc: 0.7111\n",
            "Epoch 229: train_loss: 0.0022 train_acc: 0.9991 | val_loss: 1.7114 val_acc: 0.7222\n",
            "Epoch 230: train_loss: 0.0022 train_acc: 0.9981 | val_loss: 1.7774 val_acc: 0.7074\n",
            "Epoch 231: train_loss: 0.0023 train_acc: 0.9981 | val_loss: 1.7283 val_acc: 0.7111\n",
            "Epoch 232: train_loss: 0.0012 train_acc: 1.0000 | val_loss: 1.6500 val_acc: 0.7222\n",
            "Epoch 233: train_loss: 0.0035 train_acc: 0.9972 | val_loss: 1.8654 val_acc: 0.7222\n",
            "Epoch 234: train_loss: 0.0139 train_acc: 0.9963 | val_loss: 1.7671 val_acc: 0.7407\n",
            "Epoch 235: train_loss: 0.0236 train_acc: 0.9917 | val_loss: 1.2911 val_acc: 0.7370\n",
            "Epoch 236: train_loss: 0.0127 train_acc: 0.9963 | val_loss: 1.4976 val_acc: 0.7333\n",
            "Epoch 237: train_loss: 0.0057 train_acc: 0.9991 | val_loss: 1.6065 val_acc: 0.7333\n",
            "Epoch 238: train_loss: 0.0024 train_acc: 0.9991 | val_loss: 1.5562 val_acc: 0.7296\n",
            "Epoch 239: train_loss: 0.0023 train_acc: 0.9991 | val_loss: 1.5834 val_acc: 0.7296\n",
            "Epoch 240: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 1.5913 val_acc: 0.7296\n",
            "Epoch 241: train_loss: 0.0034 train_acc: 0.9981 | val_loss: 1.6748 val_acc: 0.7296\n",
            "Epoch 242: train_loss: 0.0030 train_acc: 0.9991 | val_loss: 1.6604 val_acc: 0.7222\n",
            "Epoch 243: train_loss: 0.0025 train_acc: 0.9981 | val_loss: 1.7005 val_acc: 0.7222\n",
            "Epoch 244: train_loss: 0.0025 train_acc: 0.9991 | val_loss: 1.7115 val_acc: 0.7296\n",
            "Epoch 245: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 1.7199 val_acc: 0.7296\n",
            "Epoch 246: train_loss: 0.0048 train_acc: 0.9991 | val_loss: 1.6889 val_acc: 0.7481\n",
            "Epoch 247: train_loss: 0.0060 train_acc: 0.9981 | val_loss: 1.6171 val_acc: 0.7333\n",
            "Epoch 248: train_loss: 0.0042 train_acc: 0.9991 | val_loss: 1.6971 val_acc: 0.7222\n",
            "Epoch 249: train_loss: 0.0020 train_acc: 0.9991 | val_loss: 1.6965 val_acc: 0.7222\n",
            "Epoch 250: train_loss: 0.0021 train_acc: 0.9991 | val_loss: 1.7745 val_acc: 0.7370\n",
            "Epoch 251: train_loss: 0.0024 train_acc: 0.9981 | val_loss: 1.7358 val_acc: 0.7333\n",
            "Epoch 252: train_loss: 0.0050 train_acc: 0.9981 | val_loss: 1.6530 val_acc: 0.7148\n",
            "Epoch 253: train_loss: 0.0043 train_acc: 0.9981 | val_loss: 1.6768 val_acc: 0.7370\n",
            "Epoch 254: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.8268 val_acc: 0.7074\n",
            "Epoch 255: train_loss: 0.0065 train_acc: 0.9981 | val_loss: 1.8296 val_acc: 0.7037\n",
            "Epoch 256: train_loss: 0.0020 train_acc: 0.9981 | val_loss: 1.8143 val_acc: 0.7185\n",
            "Epoch 257: train_loss: 0.0044 train_acc: 0.9981 | val_loss: 1.7097 val_acc: 0.7259\n",
            "Epoch 258: train_loss: 0.0014 train_acc: 0.9991 | val_loss: 1.7344 val_acc: 0.7074\n",
            "Epoch 259: train_loss: 0.0048 train_acc: 0.9991 | val_loss: 1.8970 val_acc: 0.7148\n",
            "Epoch 260: train_loss: 0.0007 train_acc: 1.0000 | val_loss: 1.7782 val_acc: 0.7370\n",
            "Epoch 261: train_loss: 0.0035 train_acc: 0.9981 | val_loss: 1.8345 val_acc: 0.7222\n",
            "Epoch 262: train_loss: 0.0025 train_acc: 0.9981 | val_loss: 1.7827 val_acc: 0.7333\n",
            "Epoch 263: train_loss: 0.0023 train_acc: 0.9991 | val_loss: 1.8282 val_acc: 0.7370\n",
            "Epoch 264: train_loss: 0.0043 train_acc: 0.9991 | val_loss: 1.8675 val_acc: 0.7148\n",
            "Epoch 265: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.7064 val_acc: 0.7222\n",
            "Epoch 266: train_loss: 0.0006 train_acc: 1.0000 | val_loss: 1.7979 val_acc: 0.7037\n",
            "Epoch 267: train_loss: 0.0050 train_acc: 0.9981 | val_loss: 1.7507 val_acc: 0.7185\n",
            "Epoch 268: train_loss: 0.0022 train_acc: 0.9981 | val_loss: 1.8487 val_acc: 0.7296\n",
            "Epoch 269: train_loss: 0.0008 train_acc: 1.0000 | val_loss: 1.7678 val_acc: 0.7259\n",
            "Epoch 270: train_loss: 0.0031 train_acc: 0.9981 | val_loss: 1.8988 val_acc: 0.7111\n",
            "Epoch 271: train_loss: 0.0013 train_acc: 0.9991 | val_loss: 1.8813 val_acc: 0.7111\n",
            "Epoch 272: train_loss: 0.0044 train_acc: 0.9991 | val_loss: 1.8462 val_acc: 0.6963\n",
            "Epoch 273: train_loss: 0.0015 train_acc: 1.0000 | val_loss: 1.7375 val_acc: 0.7296\n",
            "Epoch 274: train_loss: 0.0008 train_acc: 1.0000 | val_loss: 1.7582 val_acc: 0.7111\n",
            "Epoch 275: train_loss: 0.0044 train_acc: 0.9991 | val_loss: 1.7712 val_acc: 0.7222\n",
            "Epoch 276: train_loss: 0.0034 train_acc: 0.9981 | val_loss: 1.7998 val_acc: 0.7148\n",
            "Epoch 277: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 2.0156 val_acc: 0.7222\n",
            "Epoch 278: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.8704 val_acc: 0.7037\n",
            "Epoch 279: train_loss: 0.0010 train_acc: 1.0000 | val_loss: 1.8153 val_acc: 0.7222\n",
            "Epoch 280: train_loss: 0.0042 train_acc: 0.9981 | val_loss: 1.8549 val_acc: 0.7222\n",
            "Epoch 281: train_loss: 0.0034 train_acc: 0.9981 | val_loss: 1.8066 val_acc: 0.6815\n",
            "Epoch 282: train_loss: 0.0033 train_acc: 0.9991 | val_loss: 1.9250 val_acc: 0.7111\n",
            "Epoch 283: train_loss: 0.0039 train_acc: 0.9981 | val_loss: 1.8193 val_acc: 0.7111\n",
            "Epoch 284: train_loss: 0.0034 train_acc: 0.9981 | val_loss: 1.8540 val_acc: 0.7444\n",
            "Epoch 285: train_loss: 0.0036 train_acc: 0.9981 | val_loss: 1.9002 val_acc: 0.7111\n",
            "Epoch 286: train_loss: 0.0012 train_acc: 1.0000 | val_loss: 1.8374 val_acc: 0.7148\n",
            "Epoch 287: train_loss: 0.0010 train_acc: 0.9991 | val_loss: 1.9762 val_acc: 0.7333\n",
            "Epoch 288: train_loss: 0.0067 train_acc: 0.9981 | val_loss: 1.9485 val_acc: 0.7222\n",
            "Epoch 289: train_loss: 0.0030 train_acc: 0.9991 | val_loss: 1.9643 val_acc: 0.6852\n",
            "Epoch 290: train_loss: 0.0014 train_acc: 0.9991 | val_loss: 1.9972 val_acc: 0.7333\n",
            "Epoch 291: train_loss: 0.0008 train_acc: 1.0000 | val_loss: 1.8760 val_acc: 0.7148\n",
            "Epoch 292: train_loss: 0.0025 train_acc: 0.9981 | val_loss: 1.7525 val_acc: 0.7111\n",
            "Epoch 293: train_loss: 0.0028 train_acc: 0.9991 | val_loss: 1.9535 val_acc: 0.7259\n",
            "Epoch 294: train_loss: 0.0011 train_acc: 1.0000 | val_loss: 1.9306 val_acc: 0.7074\n",
            "Epoch 295: train_loss: 0.0024 train_acc: 0.9991 | val_loss: 2.1597 val_acc: 0.7111\n",
            "Epoch 296: train_loss: 0.0008 train_acc: 1.0000 | val_loss: 1.8636 val_acc: 0.7148\n",
            "Epoch 297: train_loss: 0.0056 train_acc: 0.9981 | val_loss: 1.8870 val_acc: 0.7296\n",
            "Epoch 298: train_loss: 0.0028 train_acc: 0.9991 | val_loss: 2.0124 val_acc: 0.7222\n",
            "Epoch 299: train_loss: 0.0015 train_acc: 0.9991 | val_loss: 1.9098 val_acc: 0.7074\n",
            "Epoch 300: train_loss: 0.0026 train_acc: 0.9991 | val_loss: 2.0739 val_acc: 0.7185\n",
            "Epoch 301: train_loss: 0.0034 train_acc: 0.9981 | val_loss: 2.0537 val_acc: 0.7074\n",
            "Epoch 302: train_loss: 0.0011 train_acc: 0.9991 | val_loss: 2.0162 val_acc: 0.7259\n",
            "Epoch 303: train_loss: 0.0018 train_acc: 0.9991 | val_loss: 2.1202 val_acc: 0.7111\n",
            "Epoch 304: train_loss: 0.0027 train_acc: 0.9991 | val_loss: 1.9655 val_acc: 0.7259\n",
            "Epoch 305: train_loss: 0.0030 train_acc: 0.9991 | val_loss: 1.9383 val_acc: 0.7259\n",
            "Epoch 306: train_loss: 0.0027 train_acc: 0.9991 | val_loss: 2.0076 val_acc: 0.7259\n",
            "Epoch 307: train_loss: 0.0028 train_acc: 0.9991 | val_loss: 2.0411 val_acc: 0.7148\n",
            "Epoch 308: train_loss: 0.0004 train_acc: 1.0000 | val_loss: 1.9537 val_acc: 0.7148\n",
            "Epoch 309: train_loss: 0.0032 train_acc: 0.9981 | val_loss: 1.9577 val_acc: 0.7185\n",
            "Epoch 310: train_loss: 0.0013 train_acc: 0.9991 | val_loss: 2.0119 val_acc: 0.7407\n",
            "Epoch 311: train_loss: 0.0020 train_acc: 0.9981 | val_loss: 2.0912 val_acc: 0.7185\n",
            "Epoch 312: train_loss: 0.0009 train_acc: 0.9991 | val_loss: 1.9476 val_acc: 0.7148\n",
            "Epoch 313: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 2.1007 val_acc: 0.7111\n",
            "Epoch 314: train_loss: 0.0011 train_acc: 0.9991 | val_loss: 2.1480 val_acc: 0.7111\n",
            "Epoch 315: train_loss: 0.0008 train_acc: 1.0000 | val_loss: 2.0304 val_acc: 0.7259\n",
            "Epoch 316: train_loss: 0.0011 train_acc: 0.9991 | val_loss: 1.9514 val_acc: 0.7148\n",
            "Epoch 317: train_loss: 0.0038 train_acc: 0.9981 | val_loss: 2.0560 val_acc: 0.7111\n",
            "Epoch 318: train_loss: 0.0018 train_acc: 0.9981 | val_loss: 1.9746 val_acc: 0.7111\n",
            "Epoch 319: train_loss: 0.0474 train_acc: 0.9861 | val_loss: 1.4421 val_acc: 0.7185\n",
            "Epoch 320: train_loss: 0.0420 train_acc: 0.9870 | val_loss: 1.1718 val_acc: 0.7333\n",
            "Epoch 321: train_loss: 0.0616 train_acc: 0.9852 | val_loss: 1.7582 val_acc: 0.7296\n",
            "Epoch 322: train_loss: 0.0389 train_acc: 0.9879 | val_loss: 1.1983 val_acc: 0.7037\n",
            "Epoch 323: train_loss: 0.0105 train_acc: 0.9981 | val_loss: 1.3098 val_acc: 0.7556\n",
            "Epoch 324: train_loss: 0.0040 train_acc: 0.9991 | val_loss: 1.3082 val_acc: 0.7444\n",
            "Epoch 325: train_loss: 0.0042 train_acc: 0.9972 | val_loss: 1.3046 val_acc: 0.7296\n",
            "Epoch 326: train_loss: 0.0022 train_acc: 0.9991 | val_loss: 1.4189 val_acc: 0.7296\n",
            "Epoch 327: train_loss: 0.0036 train_acc: 0.9981 | val_loss: 1.4214 val_acc: 0.7444\n",
            "Epoch 328: train_loss: 0.0055 train_acc: 0.9981 | val_loss: 1.5610 val_acc: 0.7519\n",
            "Epoch 329: train_loss: 0.0022 train_acc: 0.9991 | val_loss: 1.4683 val_acc: 0.7630\n",
            "Epoch 330: train_loss: 0.0018 train_acc: 1.0000 | val_loss: 1.4855 val_acc: 0.7444\n",
            "Epoch 331: train_loss: 0.0027 train_acc: 0.9991 | val_loss: 1.5088 val_acc: 0.7444\n",
            "Epoch 332: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.5240 val_acc: 0.7519\n",
            "Epoch 333: train_loss: 0.0042 train_acc: 0.9981 | val_loss: 1.5199 val_acc: 0.7630\n",
            "Epoch 334: train_loss: 0.0010 train_acc: 1.0000 | val_loss: 1.5786 val_acc: 0.7667\n",
            "Epoch 335: train_loss: 0.0051 train_acc: 0.9981 | val_loss: 1.5768 val_acc: 0.7519\n",
            "Epoch 336: train_loss: 0.0028 train_acc: 0.9991 | val_loss: 1.6379 val_acc: 0.7630\n",
            "Epoch 337: train_loss: 0.0020 train_acc: 0.9981 | val_loss: 1.7564 val_acc: 0.7481\n",
            "Epoch 338: train_loss: 0.0029 train_acc: 0.9981 | val_loss: 1.6073 val_acc: 0.7444\n",
            "Epoch 339: train_loss: 0.0021 train_acc: 0.9991 | val_loss: 1.5625 val_acc: 0.7481\n",
            "Epoch 340: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 1.6661 val_acc: 0.7333\n",
            "Epoch 341: train_loss: 0.0018 train_acc: 0.9991 | val_loss: 1.6364 val_acc: 0.7519\n",
            "Epoch 342: train_loss: 0.0021 train_acc: 0.9991 | val_loss: 1.6937 val_acc: 0.7370\n",
            "Epoch 343: train_loss: 0.0025 train_acc: 0.9981 | val_loss: 1.5932 val_acc: 0.7593\n",
            "Epoch 344: train_loss: 0.0032 train_acc: 0.9991 | val_loss: 1.6999 val_acc: 0.7444\n",
            "Epoch 345: train_loss: 0.0030 train_acc: 0.9981 | val_loss: 1.6687 val_acc: 0.7370\n",
            "Epoch 346: train_loss: 0.0085 train_acc: 0.9981 | val_loss: 1.7788 val_acc: 0.7481\n",
            "Epoch 347: train_loss: 0.0029 train_acc: 0.9991 | val_loss: 1.6793 val_acc: 0.7667\n",
            "Epoch 348: train_loss: 0.0063 train_acc: 0.9981 | val_loss: 1.8314 val_acc: 0.7111\n",
            "Epoch 349: train_loss: 0.0032 train_acc: 0.9991 | val_loss: 1.6669 val_acc: 0.7333\n",
            "Epoch 350: train_loss: 0.0044 train_acc: 0.9991 | val_loss: 1.7251 val_acc: 0.7444\n",
            "Epoch 351: train_loss: 0.0011 train_acc: 0.9991 | val_loss: 1.7603 val_acc: 0.7296\n",
            "Epoch 352: train_loss: 0.0015 train_acc: 0.9991 | val_loss: 1.7525 val_acc: 0.7296\n",
            "Epoch 353: train_loss: 0.0039 train_acc: 0.9991 | val_loss: 1.8000 val_acc: 0.7333\n",
            "Epoch 354: train_loss: 0.0029 train_acc: 0.9991 | val_loss: 1.7308 val_acc: 0.7407\n",
            "Epoch 355: train_loss: 0.0010 train_acc: 1.0000 | val_loss: 1.6958 val_acc: 0.7333\n",
            "Epoch 356: train_loss: 0.0035 train_acc: 0.9981 | val_loss: 1.7875 val_acc: 0.7407\n",
            "Epoch 357: train_loss: 0.0024 train_acc: 0.9991 | val_loss: 1.7039 val_acc: 0.7481\n",
            "Epoch 358: train_loss: 0.0016 train_acc: 0.9991 | val_loss: 1.8221 val_acc: 0.7593\n",
            "Epoch 359: train_loss: 0.0010 train_acc: 1.0000 | val_loss: 1.7576 val_acc: 0.7481\n",
            "Epoch 360: train_loss: 0.0025 train_acc: 0.9981 | val_loss: 1.8354 val_acc: 0.7519\n",
            "Epoch 361: train_loss: 0.0008 train_acc: 1.0000 | val_loss: 1.7796 val_acc: 0.7333\n",
            "Epoch 362: train_loss: 0.0010 train_acc: 0.9991 | val_loss: 1.7743 val_acc: 0.7185\n",
            "Epoch 363: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 1.8983 val_acc: 0.7444\n",
            "Epoch 364: train_loss: 0.0019 train_acc: 0.9991 | val_loss: 1.7219 val_acc: 0.7481\n",
            "Epoch 365: train_loss: 0.0042 train_acc: 0.9981 | val_loss: 1.8429 val_acc: 0.7222\n",
            "Epoch 366: train_loss: 0.0013 train_acc: 0.9991 | val_loss: 1.8593 val_acc: 0.7148\n",
            "Epoch 367: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 1.8961 val_acc: 0.7296\n",
            "Epoch 368: train_loss: 0.0018 train_acc: 0.9991 | val_loss: 1.9016 val_acc: 0.7370\n",
            "Epoch 369: train_loss: 0.0011 train_acc: 1.0000 | val_loss: 1.8711 val_acc: 0.7556\n",
            "Epoch 370: train_loss: 0.0003 train_acc: 1.0000 | val_loss: 1.9696 val_acc: 0.7370\n",
            "Epoch 371: train_loss: 0.0029 train_acc: 0.9981 | val_loss: 1.9845 val_acc: 0.7444\n",
            "Epoch 372: train_loss: 0.0006 train_acc: 1.0000 | val_loss: 1.8673 val_acc: 0.7370\n",
            "Epoch 373: train_loss: 0.0048 train_acc: 0.9981 | val_loss: 1.9192 val_acc: 0.7296\n",
            "Epoch 374: train_loss: 0.0044 train_acc: 0.9981 | val_loss: 1.8873 val_acc: 0.7296\n",
            "Epoch 375: train_loss: 0.0019 train_acc: 0.9991 | val_loss: 1.9045 val_acc: 0.7259\n",
            "Epoch 376: train_loss: 0.0013 train_acc: 0.9991 | val_loss: 1.9436 val_acc: 0.7444\n",
            "Epoch 377: train_loss: 0.0063 train_acc: 0.9981 | val_loss: 1.9172 val_acc: 0.7296\n",
            "Epoch 378: train_loss: 0.0039 train_acc: 0.9991 | val_loss: 1.9331 val_acc: 0.7222\n",
            "Epoch 379: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.9038 val_acc: 0.7407\n",
            "Epoch 380: train_loss: 0.0027 train_acc: 0.9991 | val_loss: 1.8312 val_acc: 0.7370\n",
            "Epoch 381: train_loss: 0.0061 train_acc: 0.9991 | val_loss: 1.8535 val_acc: 0.7370\n",
            "Epoch 382: train_loss: 0.0018 train_acc: 0.9991 | val_loss: 1.8319 val_acc: 0.7407\n",
            "Epoch 383: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 1.9408 val_acc: 0.7333\n",
            "Epoch 384: train_loss: 0.0011 train_acc: 1.0000 | val_loss: 1.8953 val_acc: 0.7296\n",
            "Epoch 385: train_loss: 0.0009 train_acc: 1.0000 | val_loss: 1.8302 val_acc: 0.7556\n",
            "Epoch 386: train_loss: 0.0026 train_acc: 0.9981 | val_loss: 1.8426 val_acc: 0.7333\n",
            "Epoch 387: train_loss: 0.0013 train_acc: 0.9991 | val_loss: 1.9132 val_acc: 0.7407\n",
            "Epoch 388: train_loss: 0.0011 train_acc: 0.9991 | val_loss: 1.9156 val_acc: 0.7370\n",
            "Epoch 389: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.8776 val_acc: 0.7370\n",
            "Epoch 390: train_loss: 0.0042 train_acc: 0.9981 | val_loss: 1.9357 val_acc: 0.7222\n",
            "Epoch 391: train_loss: 0.0023 train_acc: 0.9991 | val_loss: 1.8944 val_acc: 0.7296\n",
            "Epoch 392: train_loss: 0.0015 train_acc: 0.9991 | val_loss: 1.9679 val_acc: 0.7111\n",
            "Epoch 393: train_loss: 0.0019 train_acc: 0.9981 | val_loss: 1.8119 val_acc: 0.7481\n",
            "Epoch 394: train_loss: 0.0020 train_acc: 0.9991 | val_loss: 1.8851 val_acc: 0.7481\n",
            "Epoch 395: train_loss: 0.0040 train_acc: 0.9991 | val_loss: 1.9459 val_acc: 0.7222\n",
            "Epoch 396: train_loss: 0.0020 train_acc: 0.9991 | val_loss: 1.8427 val_acc: 0.7296\n",
            "Epoch 397: train_loss: 0.0008 train_acc: 0.9991 | val_loss: 1.8692 val_acc: 0.7333\n",
            "Epoch 398: train_loss: 0.0018 train_acc: 0.9991 | val_loss: 1.8990 val_acc: 0.7185\n",
            "Epoch 399: train_loss: 0.0036 train_acc: 0.9991 | val_loss: 1.8973 val_acc: 0.7333\n",
            "Epoch 400: train_loss: 0.0009 train_acc: 0.9991 | val_loss: 1.7676 val_acc: 0.7444\n",
            "Epoch 401: train_loss: 0.0019 train_acc: 0.9991 | val_loss: 1.9209 val_acc: 0.7185\n",
            "Epoch 402: train_loss: 0.0007 train_acc: 1.0000 | val_loss: 2.0082 val_acc: 0.7111\n",
            "Epoch 403: train_loss: 0.0028 train_acc: 0.9981 | val_loss: 1.9130 val_acc: 0.7185\n",
            "Epoch 404: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.9433 val_acc: 0.7296\n",
            "Epoch 405: train_loss: 0.0015 train_acc: 0.9991 | val_loss: 1.9218 val_acc: 0.7222\n",
            "Epoch 406: train_loss: 0.0006 train_acc: 1.0000 | val_loss: 1.8814 val_acc: 0.7222\n",
            "Epoch 407: train_loss: 0.0023 train_acc: 0.9991 | val_loss: 2.0099 val_acc: 0.7185\n",
            "Epoch 408: train_loss: 0.0048 train_acc: 0.9991 | val_loss: 2.0443 val_acc: 0.7148\n",
            "Epoch 409: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.8787 val_acc: 0.7222\n",
            "Epoch 410: train_loss: 0.0019 train_acc: 0.9991 | val_loss: 2.0536 val_acc: 0.7407\n",
            "Epoch 411: train_loss: 0.0028 train_acc: 0.9991 | val_loss: 2.0188 val_acc: 0.7222\n",
            "Epoch 412: train_loss: 0.0025 train_acc: 0.9991 | val_loss: 2.1129 val_acc: 0.7074\n",
            "Epoch 413: train_loss: 0.0003 train_acc: 1.0000 | val_loss: 1.9891 val_acc: 0.7296\n",
            "Epoch 414: train_loss: 0.0004 train_acc: 1.0000 | val_loss: 1.9846 val_acc: 0.7111\n",
            "Epoch 415: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.9002 val_acc: 0.7333\n",
            "Epoch 416: train_loss: 0.0053 train_acc: 0.9981 | val_loss: 1.9119 val_acc: 0.7296\n",
            "Epoch 417: train_loss: 0.0022 train_acc: 0.9991 | val_loss: 1.9275 val_acc: 0.7444\n",
            "Epoch 418: train_loss: 0.0002 train_acc: 1.0000 | val_loss: 2.0364 val_acc: 0.7370\n",
            "Epoch 419: train_loss: 0.0046 train_acc: 0.9981 | val_loss: 1.9835 val_acc: 0.7296\n",
            "Epoch 420: train_loss: 0.0003 train_acc: 1.0000 | val_loss: 2.0607 val_acc: 0.7185\n",
            "Epoch 421: train_loss: 0.0062 train_acc: 0.9981 | val_loss: 2.0306 val_acc: 0.7259\n",
            "Epoch 422: train_loss: 0.0039 train_acc: 0.9981 | val_loss: 1.9248 val_acc: 0.7259\n",
            "Epoch 423: train_loss: 0.0037 train_acc: 0.9991 | val_loss: 1.9489 val_acc: 0.7370\n",
            "Epoch 424: train_loss: 0.0004 train_acc: 1.0000 | val_loss: 1.8379 val_acc: 0.7481\n",
            "Epoch 425: train_loss: 0.0045 train_acc: 0.9981 | val_loss: 1.8826 val_acc: 0.7222\n",
            "Epoch 426: train_loss: 0.0030 train_acc: 0.9991 | val_loss: 1.9484 val_acc: 0.7222\n",
            "Epoch 427: train_loss: 0.0035 train_acc: 0.9991 | val_loss: 1.8934 val_acc: 0.7185\n",
            "Epoch 428: train_loss: 0.0032 train_acc: 0.9991 | val_loss: 1.9290 val_acc: 0.7444\n",
            "Epoch 429: train_loss: 0.0025 train_acc: 0.9981 | val_loss: 1.9214 val_acc: 0.7185\n",
            "Epoch 430: train_loss: 0.0030 train_acc: 0.9991 | val_loss: 1.8393 val_acc: 0.7333\n",
            "Epoch 431: train_loss: 0.0034 train_acc: 0.9981 | val_loss: 1.7724 val_acc: 0.7333\n",
            "Epoch 432: train_loss: 0.0008 train_acc: 1.0000 | val_loss: 1.8110 val_acc: 0.7481\n",
            "Epoch 433: train_loss: 0.0032 train_acc: 0.9981 | val_loss: 1.9357 val_acc: 0.7333\n",
            "Epoch 434: train_loss: 0.0028 train_acc: 0.9991 | val_loss: 2.0165 val_acc: 0.7259\n",
            "Epoch 435: train_loss: 0.0004 train_acc: 1.0000 | val_loss: 1.8837 val_acc: 0.7296\n",
            "Epoch 436: train_loss: 0.0018 train_acc: 0.9991 | val_loss: 1.9765 val_acc: 0.7296\n",
            "Epoch 437: train_loss: 0.0041 train_acc: 0.9981 | val_loss: 1.8765 val_acc: 0.7444\n",
            "Epoch 438: train_loss: 0.0629 train_acc: 0.9814 | val_loss: 1.6613 val_acc: 0.7259\n",
            "Epoch 439: train_loss: 0.0449 train_acc: 0.9879 | val_loss: 1.1124 val_acc: 0.7259\n",
            "Epoch 440: train_loss: 0.0087 train_acc: 0.9972 | val_loss: 1.4427 val_acc: 0.7370\n",
            "Epoch 441: train_loss: 0.0038 train_acc: 1.0000 | val_loss: 1.3628 val_acc: 0.7370\n",
            "Epoch 442: train_loss: 0.0032 train_acc: 0.9991 | val_loss: 1.4935 val_acc: 0.7407\n",
            "Epoch 443: train_loss: 0.0033 train_acc: 0.9991 | val_loss: 1.4950 val_acc: 0.7259\n",
            "Epoch 444: train_loss: 0.0017 train_acc: 1.0000 | val_loss: 1.5109 val_acc: 0.7370\n",
            "Epoch 445: train_loss: 0.0028 train_acc: 0.9991 | val_loss: 1.4697 val_acc: 0.7481\n",
            "Epoch 446: train_loss: 0.0044 train_acc: 0.9991 | val_loss: 1.5070 val_acc: 0.7407\n",
            "Epoch 447: train_loss: 0.0016 train_acc: 0.9991 | val_loss: 1.6093 val_acc: 0.7296\n",
            "Epoch 448: train_loss: 0.0006 train_acc: 1.0000 | val_loss: 1.4779 val_acc: 0.7407\n",
            "Epoch 449: train_loss: 0.0013 train_acc: 1.0000 | val_loss: 1.5506 val_acc: 0.7407\n",
            "Epoch 450: train_loss: 0.0046 train_acc: 0.9991 | val_loss: 1.5809 val_acc: 0.7333\n",
            "Epoch 451: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.5970 val_acc: 0.7407\n",
            "Epoch 452: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.6319 val_acc: 0.7407\n",
            "Epoch 453: train_loss: 0.0023 train_acc: 0.9991 | val_loss: 1.6227 val_acc: 0.7296\n",
            "Epoch 454: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 1.5389 val_acc: 0.7556\n",
            "Epoch 455: train_loss: 0.0034 train_acc: 0.9981 | val_loss: 1.6240 val_acc: 0.7444\n",
            "Epoch 456: train_loss: 0.0023 train_acc: 0.9991 | val_loss: 1.7908 val_acc: 0.7370\n",
            "Epoch 457: train_loss: 0.0044 train_acc: 0.9981 | val_loss: 1.6629 val_acc: 0.7556\n",
            "Epoch 458: train_loss: 0.0039 train_acc: 0.9991 | val_loss: 1.6338 val_acc: 0.7259\n",
            "Epoch 459: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.7510 val_acc: 0.7259\n",
            "Epoch 460: train_loss: 0.0050 train_acc: 0.9981 | val_loss: 1.6904 val_acc: 0.7481\n",
            "Epoch 461: train_loss: 0.0034 train_acc: 0.9991 | val_loss: 1.5899 val_acc: 0.7519\n",
            "Epoch 462: train_loss: 0.0014 train_acc: 1.0000 | val_loss: 1.8346 val_acc: 0.7185\n",
            "Epoch 463: train_loss: 0.0034 train_acc: 0.9981 | val_loss: 1.6494 val_acc: 0.7481\n",
            "Epoch 464: train_loss: 0.0008 train_acc: 1.0000 | val_loss: 1.5953 val_acc: 0.7407\n",
            "Epoch 465: train_loss: 0.0038 train_acc: 0.9981 | val_loss: 1.7474 val_acc: 0.7333\n",
            "Epoch 466: train_loss: 0.0043 train_acc: 0.9981 | val_loss: 1.7094 val_acc: 0.7333\n",
            "Epoch 467: train_loss: 0.0026 train_acc: 0.9991 | val_loss: 1.6845 val_acc: 0.7444\n",
            "Epoch 468: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 1.7264 val_acc: 0.7407\n",
            "Epoch 469: train_loss: 0.0046 train_acc: 0.9981 | val_loss: 1.6593 val_acc: 0.7185\n",
            "Epoch 470: train_loss: 0.0013 train_acc: 0.9991 | val_loss: 1.6335 val_acc: 0.7370\n",
            "Epoch 471: train_loss: 0.0023 train_acc: 0.9991 | val_loss: 1.7140 val_acc: 0.7630\n",
            "Epoch 472: train_loss: 0.0012 train_acc: 1.0000 | val_loss: 1.6598 val_acc: 0.7481\n",
            "Epoch 473: train_loss: 0.0012 train_acc: 0.9991 | val_loss: 1.8562 val_acc: 0.7148\n",
            "Epoch 474: train_loss: 0.0051 train_acc: 0.9972 | val_loss: 2.0062 val_acc: 0.7407\n",
            "Epoch 475: train_loss: 0.0323 train_acc: 0.9879 | val_loss: 1.6025 val_acc: 0.7593\n",
            "Epoch 476: train_loss: 0.0237 train_acc: 0.9879 | val_loss: 1.4422 val_acc: 0.7074\n",
            "Epoch 477: train_loss: 0.0133 train_acc: 0.9954 | val_loss: 1.5590 val_acc: 0.7222\n",
            "Epoch 478: train_loss: 0.0150 train_acc: 0.9954 | val_loss: 1.9579 val_acc: 0.7222\n",
            "Epoch 479: train_loss: 0.0133 train_acc: 0.9972 | val_loss: 1.4960 val_acc: 0.7259\n",
            "Epoch 480: train_loss: 0.0017 train_acc: 0.9991 | val_loss: 1.6588 val_acc: 0.7259\n",
            "Epoch 481: train_loss: 0.0025 train_acc: 0.9991 | val_loss: 1.5994 val_acc: 0.7148\n",
            "Epoch 482: train_loss: 0.0023 train_acc: 0.9991 | val_loss: 1.5585 val_acc: 0.7148\n",
            "Epoch 483: train_loss: 0.0020 train_acc: 0.9991 | val_loss: 1.5919 val_acc: 0.7296\n",
            "Epoch 484: train_loss: 0.0045 train_acc: 0.9981 | val_loss: 1.6741 val_acc: 0.7296\n",
            "Epoch 485: train_loss: 0.0017 train_acc: 1.0000 | val_loss: 1.6712 val_acc: 0.7296\n",
            "Epoch 486: train_loss: 0.0021 train_acc: 0.9991 | val_loss: 1.7386 val_acc: 0.7259\n",
            "Epoch 487: train_loss: 0.0029 train_acc: 0.9991 | val_loss: 1.8322 val_acc: 0.7148\n",
            "Epoch 488: train_loss: 0.0032 train_acc: 0.9963 | val_loss: 1.7122 val_acc: 0.7148\n",
            "Epoch 489: train_loss: 0.0020 train_acc: 0.9991 | val_loss: 1.7301 val_acc: 0.7185\n",
            "Epoch 490: train_loss: 0.0026 train_acc: 0.9991 | val_loss: 1.7309 val_acc: 0.7037\n",
            "Epoch 491: train_loss: 0.0014 train_acc: 0.9991 | val_loss: 1.7245 val_acc: 0.7259\n",
            "Epoch 492: train_loss: 0.0042 train_acc: 0.9991 | val_loss: 1.7664 val_acc: 0.7111\n",
            "Epoch 493: train_loss: 0.0006 train_acc: 1.0000 | val_loss: 1.7608 val_acc: 0.7111\n",
            "Epoch 494: train_loss: 0.0055 train_acc: 0.9991 | val_loss: 1.8517 val_acc: 0.7370\n",
            "Epoch 495: train_loss: 0.0007 train_acc: 1.0000 | val_loss: 1.7822 val_acc: 0.7148\n",
            "Epoch 496: train_loss: 0.0015 train_acc: 0.9991 | val_loss: 1.8506 val_acc: 0.7222\n",
            "Epoch 497: train_loss: 0.0041 train_acc: 0.9981 | val_loss: 1.7763 val_acc: 0.7222\n",
            "Epoch 498: train_loss: 0.0032 train_acc: 0.9981 | val_loss: 1.9074 val_acc: 0.7185\n",
            "Epoch 499: train_loss: 0.0024 train_acc: 0.9981 | val_loss: 1.7168 val_acc: 0.7185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0FwUXj1KfaN",
        "outputId": "a16b4cf8-42cf-4188-dcf9-e436cf3fb861"
      },
      "source": [
        "test_loss, test_acc = evaluate(m, iter(test_batch_it), loss_fn, len(test_batch_it))\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.682 | Test Acc: 74.26%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDsz_cIGSuKO"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0085XuZxEtlH"
      },
      "source": [
        "filename = 'Depression2.pkl'\n",
        "pickle.dump(motut4-model.ptdel, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuVIJRlnI9qI"
      },
      "source": [
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=1500)\n",
        "pickle.dump(cv, open('cv-transform.pkl', 'wb'))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQtqKN-t8xRQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}